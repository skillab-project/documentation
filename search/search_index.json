{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SKILLAB SKILLAB aims at delivering an open-source skill demand/supply identification, analysis and prediction hub for citizens, enterprises, academia, and policy makers. For more general information about the SKILLAB project visit our website . About the project The SKILLAB project aims at developing a holistic skills management and shortage identification platform that will monitor the demand of European organizations and entities for specific skillsets, the supply of skills by job-seekers and potential employees along with European policies on the subject and will propose targeted strategies, curiculla development, policies and job advancement suggestions. Highlighting the importance of workforce upskilling and vocational training by generating policies for hiring bodies, offering suggestions for improvement on existing European initiatives and providing a reliable skills forecasting and monitoring observatory. At the core of the proposed framework lies the leveraging of Artificial Intelligence technologies that will harness high-volumes of labour market, organizational and European taxonomies and initiatives data and will reduce the labour gaps and shortages on a regional, sectoral and temporal level. SKILLAB is the joint effort of 10 partners from 6 European countries. Contributing to the enhancement of the European Labour market and the Pact for Skills by delivering a hub targeted towards educational paths for upskilling and reskilling, HR strategies and guidelines for optimal candidate recruitment, recommendations for policymakers on skills shortages and mismatches and the identification of emerging skills and roles. Key Objectives Development and deployment of a holistic skills management and shortage identification platform to policymakers to automatically monitor the demand for skills in their market domain and the supply of skills, identifying skill gaps & emerging jobs. Integration of Advanced Machine Learning and Natural Language Processing methods into the analysis of competences. Generation of systematic recommendations for hiring policies, personalized educational plans for individuals aiming at upskilling and reskilling, detailed reports for Curricula Improvement targeting Higher Education Institutions (HEI) and Education Policymakers. Specification and automatic update of hard- and soft-skill taxonomies and derivation of vocational training priorities targeting Government, Policymakers and Public Bodies. Consortium The Consortium consists of the following partners: Aristotle University of Thessaloniki Centre for Research & Technology Hellas Eclipse Foundation ViLabs Versatile Innovations Humboldt-Universitaet zu Berlin Telefonica Cluj IT Intrasoft International Democritus University of Thrace University of Macedonia","title":"Home"},{"location":"#welcome-to-skillab","text":"SKILLAB aims at delivering an open-source skill demand/supply identification, analysis and prediction hub for citizens, enterprises, academia, and policy makers. For more general information about the SKILLAB project visit our website .","title":"Welcome to SKILLAB"},{"location":"#about-the-project","text":"The SKILLAB project aims at developing a holistic skills management and shortage identification platform that will monitor the demand of European organizations and entities for specific skillsets, the supply of skills by job-seekers and potential employees along with European policies on the subject and will propose targeted strategies, curiculla development, policies and job advancement suggestions. Highlighting the importance of workforce upskilling and vocational training by generating policies for hiring bodies, offering suggestions for improvement on existing European initiatives and providing a reliable skills forecasting and monitoring observatory. At the core of the proposed framework lies the leveraging of Artificial Intelligence technologies that will harness high-volumes of labour market, organizational and European taxonomies and initiatives data and will reduce the labour gaps and shortages on a regional, sectoral and temporal level. SKILLAB is the joint effort of 10 partners from 6 European countries. Contributing to the enhancement of the European Labour market and the Pact for Skills by delivering a hub targeted towards educational paths for upskilling and reskilling, HR strategies and guidelines for optimal candidate recruitment, recommendations for policymakers on skills shortages and mismatches and the identification of emerging skills and roles.","title":"About the project"},{"location":"#key-objectives","text":"Development and deployment of a holistic skills management and shortage identification platform to policymakers to automatically monitor the demand for skills in their market domain and the supply of skills, identifying skill gaps & emerging jobs. Integration of Advanced Machine Learning and Natural Language Processing methods into the analysis of competences. Generation of systematic recommendations for hiring policies, personalized educational plans for individuals aiming at upskilling and reskilling, detailed reports for Curricula Improvement targeting Higher Education Institutions (HEI) and Education Policymakers. Specification and automatic update of hard- and soft-skill taxonomies and derivation of vocational training priorities targeting Government, Policymakers and Public Bodies.","title":"Key Objectives"},{"location":"#consortium","text":"The Consortium consists of the following partners: Aristotle University of Thessaloniki Centre for Research & Technology Hellas Eclipse Foundation ViLabs Versatile Innovations Humboldt-Universitaet zu Berlin Telefonica Cluj IT Intrasoft International Democritus University of Thrace University of Macedonia","title":"Consortium"},{"location":"getting-started-admin/","text":"How to setup the platform The SKILLAB platform has 4 different deployment version: - Citizen - Industry - Acadimia - Policy Makers (Note, not yet finished!) Citizen For this intallation version the admin can use this docker-compose file. ... Industry For this intallation version the admin can use this docker-compose file. ... Acadimia For this intallation version the admin can use this docker-compose file. .. Policy Makers For this intallation version the admin can use this docker-compose file. ...","title":"How to setup the platform"},{"location":"getting-started-admin/#how-to-setup-the-platform","text":"The SKILLAB platform has 4 different deployment version: - Citizen - Industry - Acadimia - Policy Makers (Note, not yet finished!)","title":"How to setup the platform"},{"location":"getting-started-admin/#citizen","text":"For this intallation version the admin can use this docker-compose file. ...","title":"Citizen"},{"location":"getting-started-admin/#industry","text":"For this intallation version the admin can use this docker-compose file. ...","title":"Industry"},{"location":"getting-started-admin/#acadimia","text":"For this intallation version the admin can use this docker-compose file. ..","title":"Acadimia"},{"location":"getting-started-admin/#policy-makers","text":"For this intallation version the admin can use this docker-compose file. ...","title":"Policy Makers"},{"location":"getting-started-user/","text":"How to use the platform ...","title":"How to use the platform"},{"location":"getting-started-user/#how-to-use-the-platform","text":"...","title":"How to use the platform"},{"location":"components/tracker/","text":"Skillab Tracker A configurable and extensible tracker that fetches , preprocesses , stores and serves data from different sources for skill related entities such as Courses, Job Listings, Employee Profiles, Organizational Profiles and more. Skillab Tracker is a tool built to gather, preprocess, store, and deliver data across a diverse range of skill-related domains. By centralizing information from multiple sources, it enables the creation of unified datasets for comprehensive analytics and reporting. Skillab Tracker can integrate data on entities such as organizations, projects, academic papers, official project reports, online courses, job postings, internet profiles, legal policies, and law publications, providing a robust foundation for deep insights and informed decision-making. The Skillab Tracker is developed in Python, leveraging the Django Ninja framework, an extension of Django designed for building APIs with greater ease and flexibility. As outlined in the architecture section, Skillab Tracker also integrates various libraries from the Python ecosystem for networking, text parsing, data manipulation and scientific analysis, enhancing its functionality. Note: This is a stripped-down version designed specifically for testing. It includes only a database dump of records from open sources, with many modules\u2014such as data miners, skill and occupation extraction, translations, and others\u2014omitted to reduce the Docker build size and simplify installation. How to install for testing Prerequisites: Python (most likely any 3.x.x version will do) with the venv module installed Docker Installation steps: # Clone the repository git clone https://github.com/skillab-project/skillab-tracker-testathon.git # Go into the created folder cd /skillab-tracker-testathon # Create enviroment file cp .env.example.dev .env # Create python virtual enviroment and install requirements python -m venv venv source ./venv/bin/activate pip install -r requirements.txt # Run postgresql and load dump using Docker docker compose up -d # Run tests (the server doesn't need to be running) python manage.py test # Run the server in development mode python manage.py runserver Visit the Swagger Documentation page to view the available endpoints when the server is running. For more information on how to conduct testing visit the testing page .","title":"General Info"},{"location":"components/tracker/#skillab-tracker","text":"A configurable and extensible tracker that fetches , preprocesses , stores and serves data from different sources for skill related entities such as Courses, Job Listings, Employee Profiles, Organizational Profiles and more. Skillab Tracker is a tool built to gather, preprocess, store, and deliver data across a diverse range of skill-related domains. By centralizing information from multiple sources, it enables the creation of unified datasets for comprehensive analytics and reporting. Skillab Tracker can integrate data on entities such as organizations, projects, academic papers, official project reports, online courses, job postings, internet profiles, legal policies, and law publications, providing a robust foundation for deep insights and informed decision-making. The Skillab Tracker is developed in Python, leveraging the Django Ninja framework, an extension of Django designed for building APIs with greater ease and flexibility. As outlined in the architecture section, Skillab Tracker also integrates various libraries from the Python ecosystem for networking, text parsing, data manipulation and scientific analysis, enhancing its functionality. Note: This is a stripped-down version designed specifically for testing. It includes only a database dump of records from open sources, with many modules\u2014such as data miners, skill and occupation extraction, translations, and others\u2014omitted to reduce the Docker build size and simplify installation.","title":"Skillab Tracker"},{"location":"components/tracker/#how-to-install-for-testing","text":"Prerequisites: Python (most likely any 3.x.x version will do) with the venv module installed Docker Installation steps: # Clone the repository git clone https://github.com/skillab-project/skillab-tracker-testathon.git # Go into the created folder cd /skillab-tracker-testathon # Create enviroment file cp .env.example.dev .env # Create python virtual enviroment and install requirements python -m venv venv source ./venv/bin/activate pip install -r requirements.txt # Run postgresql and load dump using Docker docker compose up -d # Run tests (the server doesn't need to be running) python manage.py test # Run the server in development mode python manage.py runserver Visit the Swagger Documentation page to view the available endpoints when the server is running. For more information on how to conduct testing visit the testing page .","title":"How to install for testing"},{"location":"components/tracker/testing/","text":"Skillab Tracker Overview Automated testing via coding is the preferred approach as it ensures reproducibility and aids project maintenance, particularly during merges. However, semantic aspects of the application that cannot be tested through code should be verified manually via the documentation page. Types of testing that can be conducted Data Integrity Testing Data Integrity Testing ensures that data remains accurate, consistent, unaltered, and free from unnecessary data or duplications during storage, retrieval, and processing. Examples of test cases: * Check for Duplicate Data in API Responses. (Easy) * Validate Unique Entries. (Easy) * Verify endpoints return the same information every time when identical filters are applied. (Easy) Accuracy Testing Accuracy testing refers to the process of evaluating how close a system's output is to the correct or expected result. Examples of test cases: * Accurate set of extracted skills and occupations based on the information provided. (Hard) * Propagation and backpropagation returns expected results. (Medium) Most likely the accuracy of the extracted skills can only be conducted manually and not automated. Completeness Testing Completeness testing ensures that all required functionalities, data, or conditions in a system are fully implemented and accounted for. Examples of test cases: * Missing skills while there is information to be extracted from. (Easy) Logic Testing Ensure that the filtering logic implemented at the endpoints functions as expected. Examples of test cases: * AND/OR Logic: Confirm that the filtering operators (AND, OR) behave correctly when applied. (Easy) * Keyword Filtering: Verify that keyword-based filters return the correct results.(Easy) * ID Filtering: Ensure that an id is present in the results when it is used for filtering. (Easy) Procedure Manual Testing You can only test the application's semantic meaning manually by inspecting the API's responses. The Swagger Documentation page helps you create requests and view their results. To report a bug, submit the CURL command copied from the documentation page, optionally accompanied by a screenshot. Example of manual testing For example, by inspecting the list of returned courses, we can notice that the skills extraction for a specific course worked like expected. Or the skills are associated to the course's title and description. This is part of the Data Integrity Testing . Course Title: Security and DevOps Skills: * monitor logging operations * tools for software configuration management * DevOps * coordinate security Automated Testing For writing tests, we recommend using Django's test client along with Python's built-in unittest framework . However, you are free to use your preferred testing framework and dependencies. Example of automated testing This test validates that single-keyword searching works as expected in job listings and is part of the Logic Testing . You can also view the following test in api/tests.py . from unittest import TestCase from django.test import Client class JobsTest(TestCase): def setUp(self): super().setUp() self.client = Client() def test_jobs_keyword(self): keyword = \"software\" response = self.client.post(\"/api/jobs\", data={\"keywords\": [keyword]}) self.assertEqual(response.status_code, 200, \"Response wasn't ok.\") jobs = response.json()[\"items\"] for job in jobs: self.assertTrue( keyword in (job[\"title\"] + job[\"description\"]).lower(), f\"Some job didn't include the filtered keyword in its title or description. Job ID: {job[\"id\"]}\", ) You can run the tests by executing the following command in your terminal: python manage.py test Documentation sources for testing: * https://docs.djangoproject.com/en/5.1/topics/testing/","title":"Testing"},{"location":"components/tracker/testing/#skillab-tracker","text":"","title":"Skillab Tracker"},{"location":"components/tracker/testing/#overview","text":"Automated testing via coding is the preferred approach as it ensures reproducibility and aids project maintenance, particularly during merges. However, semantic aspects of the application that cannot be tested through code should be verified manually via the documentation page.","title":"Overview"},{"location":"components/tracker/testing/#types-of-testing-that-can-be-conducted","text":"","title":"Types of testing that can be conducted"},{"location":"components/tracker/testing/#data-integrity-testing","text":"Data Integrity Testing ensures that data remains accurate, consistent, unaltered, and free from unnecessary data or duplications during storage, retrieval, and processing. Examples of test cases: * Check for Duplicate Data in API Responses. (Easy) * Validate Unique Entries. (Easy) * Verify endpoints return the same information every time when identical filters are applied. (Easy)","title":"Data Integrity Testing"},{"location":"components/tracker/testing/#accuracy-testing","text":"Accuracy testing refers to the process of evaluating how close a system's output is to the correct or expected result. Examples of test cases: * Accurate set of extracted skills and occupations based on the information provided. (Hard) * Propagation and backpropagation returns expected results. (Medium) Most likely the accuracy of the extracted skills can only be conducted manually and not automated.","title":"Accuracy Testing"},{"location":"components/tracker/testing/#completeness-testing","text":"Completeness testing ensures that all required functionalities, data, or conditions in a system are fully implemented and accounted for. Examples of test cases: * Missing skills while there is information to be extracted from. (Easy)","title":"Completeness Testing"},{"location":"components/tracker/testing/#logic-testing","text":"Ensure that the filtering logic implemented at the endpoints functions as expected. Examples of test cases: * AND/OR Logic: Confirm that the filtering operators (AND, OR) behave correctly when applied. (Easy) * Keyword Filtering: Verify that keyword-based filters return the correct results.(Easy) * ID Filtering: Ensure that an id is present in the results when it is used for filtering. (Easy)","title":"Logic Testing"},{"location":"components/tracker/testing/#procedure","text":"","title":"Procedure"},{"location":"components/tracker/testing/#manual-testing","text":"You can only test the application's semantic meaning manually by inspecting the API's responses. The Swagger Documentation page helps you create requests and view their results. To report a bug, submit the CURL command copied from the documentation page, optionally accompanied by a screenshot.","title":"Manual Testing"},{"location":"components/tracker/testing/#example-of-manual-testing","text":"For example, by inspecting the list of returned courses, we can notice that the skills extraction for a specific course worked like expected. Or the skills are associated to the course's title and description. This is part of the Data Integrity Testing . Course Title: Security and DevOps Skills: * monitor logging operations * tools for software configuration management * DevOps * coordinate security","title":"Example of manual testing"},{"location":"components/tracker/testing/#automated-testing","text":"For writing tests, we recommend using Django's test client along with Python's built-in unittest framework . However, you are free to use your preferred testing framework and dependencies.","title":"Automated Testing"},{"location":"components/tracker/testing/#example-of-automated-testing","text":"This test validates that single-keyword searching works as expected in job listings and is part of the Logic Testing . You can also view the following test in api/tests.py . from unittest import TestCase from django.test import Client class JobsTest(TestCase): def setUp(self): super().setUp() self.client = Client() def test_jobs_keyword(self): keyword = \"software\" response = self.client.post(\"/api/jobs\", data={\"keywords\": [keyword]}) self.assertEqual(response.status_code, 200, \"Response wasn't ok.\") jobs = response.json()[\"items\"] for job in jobs: self.assertTrue( keyword in (job[\"title\"] + job[\"description\"]).lower(), f\"Some job didn't include the filtered keyword in its title or description. Job ID: {job[\"id\"]}\", ) You can run the tests by executing the following command in your terminal: python manage.py test Documentation sources for testing: * https://docs.djangoproject.com/en/5.1/topics/testing/","title":"Example of automated testing"},{"location":"components/ku-detection/","text":"KU Detection Back-End Based on https://github.com/ElisavetKanidou/KU-Detection-Back-End Description This project implements the backend API for an application designed to detect \"Knowledge Units\" (KU). It is built with Flask (Python) and provides endpoints for: Managing a list of Git repositories (Add, List, Edit, Delete). Fetching and storing commit information from repositories. Performing analysis on code files from specific commits using a pre-trained CodeBERT model. Monitoring the status and progress of the analysis process. Retrieving the analysis results (detected KUs). The backend interacts with a PostgreSQL database for data persistence and uses Git commands for cloning/updating repositories. Getting Started Guide Follow the steps below to set up the backend locally on your machine. Prerequisites Git: Installed on your system. ( Download Git ) Python: Version 3.8 or newer is recommended. ( Download Python ) Ensure pip is available. PostgreSQL: An active PostgreSQL server installation. You will need the connection details (host, port, database name, user, password). ( Download PostgreSQL ) Contribution Steps Fork the Repository: Navigate to the main repository . Click the \"Fork\" button in the top-right corner to create a copy in your own GitHub account. Clone Your Fork: Open a terminal or command prompt. Clone your fork locally, replacing <your-username> : bash git clone https://github.com/<your-username>/KU-Detection.git (Note: The repository name in your fork might be KU-Detection-Back-End if you didn't change it during the fork, or KU-Detection if you did. Adjust the command accordingly.) Navigate to the Project Directory: bash cd KU-Detection # or KU-Detection-Back-End, depending on the folder name Create and Activate a Virtual Environment (Recommended): Create a virtual environment: bash python -m venv venv Activate it: Linux/macOS: source venv/bin/activate Windows: venv\\Scripts\\activate Install Dependencies: Make sure the virtual environment is activated. Run the following command to install all necessary libraries: bash pip install -r requirements.txt Database & Environment Setup: Create a database in your PostgreSQL installation if one doesn't already exist. Create a file named .env in the project's root directory. Add the following environment variables to .env , replacing the values with your own details: dotenv DB_HOST=localhost # or your DB server address DB_PORT=5432 # or your DB server port DB_NAME=your_db_name # Your database name DB_USER=your_db_user # Your database user DB_PASSWORD=your_db_password # Your user's password CLONED_REPO_BASE_PATH=/path/to/store/cloned/repos # Directory to store cloned repos CODEBERT_BASE_PATH=/path/to/your/codebert/model # Directory containing CodeBERT model files The application will attempt to create the necessary tables ( repositories , commits , analysis_results ) on first startup, but the database and user must already exist. Configure Git Longpaths (If Required): The application attempts to enable Git long path support ( core.longpaths = true ) on startup. This might require administrator/sudo privileges the first time. Alternatively, you can configure it manually (as administrator/sudo): bash git config --system core.longpaths true CodeBERT Model: The CodeBERT model files used for analysis. You need to place these in a directory on your system. You have to download the model from here . And add it in models/codebert Running the Application Localy Set the Flask Application: In the terminal (with the virtual environment activated): Linux/macOS: export FLASK_APP=api:create_app Windows: set FLASK_APP=api:create_app (Note: api:create_app refers to the create_app function within api/__init__.py ) Start the Development Server: bash flask run The application will typically be accessible at http://127.0.0.1:5000 . You can view the list of available endpoints via the Swagger UI at http://127.0.0.1:5000/swagger . With Docker To run the application and its PostgreSQL database using Docker, ensure Docker and Docker Compose are installed. Finally, execute docker-compose up --build in the project's root directory to build and start the services in containers. Technologies Python Flask Psycopg2 (PostgreSQL Adapter) Transformers (Hugging Face) PyTorch / TensorFlow (Depending on the model) GitPython python-dotenv Flask-CORS Flask-Swagger-UI","title":"General Info"},{"location":"components/ku-detection/#ku-detection-back-end","text":"Based on https://github.com/ElisavetKanidou/KU-Detection-Back-End","title":"KU Detection Back-End"},{"location":"components/ku-detection/#description","text":"This project implements the backend API for an application designed to detect \"Knowledge Units\" (KU). It is built with Flask (Python) and provides endpoints for: Managing a list of Git repositories (Add, List, Edit, Delete). Fetching and storing commit information from repositories. Performing analysis on code files from specific commits using a pre-trained CodeBERT model. Monitoring the status and progress of the analysis process. Retrieving the analysis results (detected KUs). The backend interacts with a PostgreSQL database for data persistence and uses Git commands for cloning/updating repositories.","title":"Description"},{"location":"components/ku-detection/#getting-started-guide","text":"Follow the steps below to set up the backend locally on your machine.","title":"Getting Started Guide"},{"location":"components/ku-detection/#prerequisites","text":"Git: Installed on your system. ( Download Git ) Python: Version 3.8 or newer is recommended. ( Download Python ) Ensure pip is available. PostgreSQL: An active PostgreSQL server installation. You will need the connection details (host, port, database name, user, password). ( Download PostgreSQL )","title":"Prerequisites"},{"location":"components/ku-detection/#contribution-steps","text":"Fork the Repository: Navigate to the main repository . Click the \"Fork\" button in the top-right corner to create a copy in your own GitHub account. Clone Your Fork: Open a terminal or command prompt. Clone your fork locally, replacing <your-username> : bash git clone https://github.com/<your-username>/KU-Detection.git (Note: The repository name in your fork might be KU-Detection-Back-End if you didn't change it during the fork, or KU-Detection if you did. Adjust the command accordingly.) Navigate to the Project Directory: bash cd KU-Detection # or KU-Detection-Back-End, depending on the folder name Create and Activate a Virtual Environment (Recommended): Create a virtual environment: bash python -m venv venv Activate it: Linux/macOS: source venv/bin/activate Windows: venv\\Scripts\\activate Install Dependencies: Make sure the virtual environment is activated. Run the following command to install all necessary libraries: bash pip install -r requirements.txt Database & Environment Setup: Create a database in your PostgreSQL installation if one doesn't already exist. Create a file named .env in the project's root directory. Add the following environment variables to .env , replacing the values with your own details: dotenv DB_HOST=localhost # or your DB server address DB_PORT=5432 # or your DB server port DB_NAME=your_db_name # Your database name DB_USER=your_db_user # Your database user DB_PASSWORD=your_db_password # Your user's password CLONED_REPO_BASE_PATH=/path/to/store/cloned/repos # Directory to store cloned repos CODEBERT_BASE_PATH=/path/to/your/codebert/model # Directory containing CodeBERT model files The application will attempt to create the necessary tables ( repositories , commits , analysis_results ) on first startup, but the database and user must already exist. Configure Git Longpaths (If Required): The application attempts to enable Git long path support ( core.longpaths = true ) on startup. This might require administrator/sudo privileges the first time. Alternatively, you can configure it manually (as administrator/sudo): bash git config --system core.longpaths true CodeBERT Model: The CodeBERT model files used for analysis. You need to place these in a directory on your system. You have to download the model from here . And add it in models/codebert","title":"Contribution Steps"},{"location":"components/ku-detection/#running-the-application","text":"","title":"Running the Application"},{"location":"components/ku-detection/#localy","text":"Set the Flask Application: In the terminal (with the virtual environment activated): Linux/macOS: export FLASK_APP=api:create_app Windows: set FLASK_APP=api:create_app (Note: api:create_app refers to the create_app function within api/__init__.py ) Start the Development Server: bash flask run The application will typically be accessible at http://127.0.0.1:5000 . You can view the list of available endpoints via the Swagger UI at http://127.0.0.1:5000/swagger .","title":"Localy"},{"location":"components/ku-detection/#with-docker","text":"To run the application and its PostgreSQL database using Docker, ensure Docker and Docker Compose are installed. Finally, execute docker-compose up --build in the project's root directory to build and start the services in containers.","title":"With Docker"},{"location":"components/ku-detection/#technologies","text":"Python Flask Psycopg2 (PostgreSQL Adapter) Transformers (Hugging Face) PyTorch / TensorFlow (Depending on the model) GitPython python-dotenv Flask-CORS Flask-Swagger-UI","title":"Technologies"},{"location":"components/ku-detection/tests/","text":"Testing the API Routes Component ( api/routes.py ) 1. Introduction This document describes the testing strategy and procedures for the Flask routes component located in api/routes.py . The goal is to ensure the correct operation, robustness, and expected behavior of the API endpoints. The primary methodology used is Unit Testing utilizing Python's built-in unittest and unittest.mock libraries. 2. Testing Philosophy The tests for the routes ( api/test_routes.py ) are designed to be: Isolated: Each test focuses on a specific endpoint or operational scenario. External dependencies, primarily calls to the database ( api/data_db.py ) and time-consuming operations (like Git repo cloning or loading ML models), are bypassed (mocked) . Fast: Bypassing external dependencies allows for rapid test execution, facilitating integration into CI/CD pipelines and providing immediate feedback during development. Repeatable: Tests do not depend on the state of the actual database or file system and must yield the same result every time they are executed. Important: These unit tests do not interact with the actual PostgreSQL database . They use mocks to simulate database responses, ensuring the production database remains unaffected. 2.1 Testing Strategy For the API routes component ( api/routes.py ), the following types of testing are applied: Unit Testing: (Primary Method) Testing each API endpoint in isolation, using mocks to isolate from external dependencies (database, Git, ML model). Focuses on the internal logic of the route, request/response handling, and calling the appropriate (mocked) external functions. Functional Testing: Verifying that the endpoints behave as expected based on specifications for specific inputs and usage scenarios. Input Validation Testing: Checking the behavior of endpoints when receiving valid, invalid, boundary, or incomplete input data (URL parameters, query parameters, JSON payloads). Error Handling Testing: Ensuring the application correctly handles errors (e.g., database failure, invalid repository URL, analysis errors) and returns appropriate HTTP status codes and error messages. Asynchronous Operation Testing (Simulated): Testing the initiation and response structure (e.g., Server-Sent Events for /analyze ) of background operations by simulating their behavior with mocks. (Potential future types of testing, complementary to the existing unit tests, could include): Integration Testing (Database): Description: Testing the interaction of API routes with a real (but separate, for testing) PostgreSQL database. This would verify that data is written and read correctly, database constraints work as expected, and more complex queries (if any) return correct results. Goal: Ensure the application functions correctly in conjunction with the actual database system. Integration Testing (Git Operations): Description: Testing the Git-interacting functions ( clone_repo , pull_repo , extract_contributions , get_history_repo ) by executing real Git commands against one or more real (perhaps temporary or sample) Git repositories. Goal: Verify the application can correctly handle Git repositories, including cases like clone failure (e.g., wrong URL, private repo), history changes, etc. Integration Testing (ML Model Analysis): Description: Testing the full analysis flow ( /analyze endpoint and codebert_sliding_window ) by loading and using the actual CodeBERT model . Could check if results are produced (without necessarily judging the correctness of KUs) for sample code and if errors during model loading or execution are handled gracefully. Goal: Ensure the analysis system works end-to-end with the real ML model. These tests might be slow. End-to-End (E2E) Testing: Description: Simulating complete user scenarios from the perspective of an external client calling the API. For example: a) Add repo, b) Fetch commits, c) Start analysis, d) Check status, e) Retrieve results. Goal: Verify that the different components (routes, database, git operations, analysis) cooperate correctly to complete a full workflow. Load / Performance Testing: Description: Using tools (e.g., Locust, k6, JMeter) to simulate multiple concurrent users calling the API endpoints, especially demanding ones like /analyze and /commits . Measure response times, error rates, and resource usage (CPU, memory, database connections) under load. Goal: Identify bottlenecks, evaluate application scalability, and ensure acceptable performance under expected load. Concurrency Testing: Description: Targeted testing of scenarios where multiple operations run simultaneously, especially around the background analysis ( /analyze ). Check for potential race conditions (e.g., updating analysis status in the database from multiple threads), deadlocks, or resource exhaustion (e.g., excessive number of threads). Goal: Ensure application stability and correctness when multiple analyses run concurrently. 3. Running the Unit Tests To run the specific unit tests for the API routes: Ensure you have installed the necessary dependencies (usually via pip install -r requirements.txt in the project's root directory) and activated your virtual environment. Execute the following command from the project's root directory: bash python -m unittest api.test_routes.py 4. Existing Test Suite Overview The current tests in api/test_routes.py cover the basic functionality of the following endpoints: /commits (POST): Retrieve commits, handle clone/pull. /repos (POST): Create a new repository entry. /detected_kus (GET): Retrieve detected KUs. /repos/<repo_name> (PUT): Edit an existing repository entry. /timestamps (GET): Retrieve commit timestamps. /historytime (GET): Retrieve commit history timeline. /delete_repo/<repo_name> (DELETE): Delete a repository entry. /repos (GET): List all repository entries. /analyze (GET): Start the analysis process (streaming response). /analysis_status (GET): Retrieve analysis status. /analyzedb (GET): Retrieve stored analysis results for a repo. /analyzeall (GET): Retrieve all stored analysis results. A detailed description of each test case is provided in the Test Case Catalog below. 5. Adding New Unit Tests When adding new endpoints or modifying existing ones in api/routes.py , corresponding unit tests should also be added or updated in api/test_routes.py . Follow these steps: Create Test Function: Add a new method to the FlaskAPITests class that starts with test_ (e.g., test_new_endpoint_success ). Use @patch : Use the @patch decorator to mock all external dependencies called by your route (e.g., functions from api.data_db , core.git_operations , etc.). Target the function where it is used (usually imported into api.routes ). The mocks are passed as arguments to your test function (in reverse order of the decorators). python @patch('api.routes.some_db_function') @patch('api.routes.another_external_call') def test_new_endpoint_success(self, mock_external_call, mock_db_function): # Mocks are passed in reverse order: # mock_external_call corresponds to @patch('api.routes.another_external_call') # mock_db_function corresponds to @patch('api.routes.some_db_function') # ... Configure Mocks: Inside your test function, set the return values ( return_value ) or side effects ( side_effect ) of the mock objects to simulate various scenarios (e.g., successful data retrieval, database error, empty results). python mock_db_function.return_value = [{'id': 1, 'data': 'sample'}] mock_external_call.side_effect = Exception(\"Network Error\") Call Endpoint: Use self.client (the Flask test client, available in FlaskAPITests ) to send the appropriate HTTP request to the endpoint you want to test. python response = self.client.get('/new_endpoint?param=value') response = self.client.post('/another_endpoint', json={'key': 'value'}) Assertions: Use unittest 's assert methods (e.g., self.assertEqual , self.assertTrue , self.assertIn ) to verify: The response status code: self.assertEqual(response.status_code, 200) The content type: self.assertEqual(response.content_type, 'application/json') The response data: data = json.loads(response.data); self.assertEqual(data['message'], 'Success') Whether the mocked functions were called correctly: mock_db_function.assert_called_once_with(...) or mock_external_call.assert_called_once() 6. Test Case Catalog This catalog details the existing and proposed unit tests for the API routes. 6.1 Existing Test Cases ( api/test_routes.py ) ID: TC_LIST_COMMITS Description: Verifies the functionality of the /commits (POST) endpoint for retrieving a list of commits from a repository. Tests two scenarios: a) when the repo does not exist locally (requires clone_repo ), and b) when the repo exists (requires pull_repo ). Verifies the status code (200) and the correctness of the returned data. Also checks that the appropriate functions (clone/pull, extract, save) are called. Category: Functional Testing, Integration Testing (Simulated - Git Operations & DB) Dependencies (Mocks): api.routes.save_commits_to_db , api.routes.extract_contributions , api.routes.pull_repo , api.routes.repo_exists , api.routes.clone_repo ID: TC_CREATE_REPO Description: Verifies the functionality of the /repos (POST) endpoint for creating a new repository entry. Checks for successful creation (status 201, success message) and proper error handling during database saving (status 500, error message). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.save_repo_to_db ID: TC_GET_DETECTED_KUS Description: Verifies the functionality of the /detected_kus (GET) endpoint for retrieving the list of detected knowledge units. Checks for successful retrieval (status 200, data check), the case where the database returns no data (status 500), and handling of database exceptions (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.getdetected_kus ID: TC_EDIT_REPO Description: Verifies the functionality of the /repos/<repo_name> (PUT) endpoint for updating information of an existing repository. Checks for successful update (status 200, success message) and error handling during database saving (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.save_repo_to_db ID: TC_GET_TIMESTAMPS Description: Verifies the functionality of the /timestamps (GET) endpoint for retrieving commit timestamps for a specific repository. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and handling cases where the database returns no data (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_commits_timestamps_from_db ID: TC_HISTORYTIME Description: Verifies the functionality of the /historytime (GET) endpoint for retrieving the timeline of commit dates for a repository. Checks for successful retrieval (status 200, structure and data check), the requirement of the repo_url parameter (status 400), and handling exceptions during history retrieval (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_history_repo ID: TC_DELETE_REPO Description: Verifies the functionality of the /delete_repo/<repo_name> (DELETE) endpoint for deleting a repository entry. Checks for successful deletion (status 200, success message) and handling exceptions during database deletion (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.delete_repo_from_db ID: TC_LIST_REPOS Description: Verifies the functionality of the /repos (GET) endpoint for retrieving the list of all repositories. Checks for successful retrieval (status 200, structure and data check) and handling of database exceptions (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.get_all_repos_from_db ID: TC_ANALYZE_ENDPOINT Description: Verifies the functionality of the /analyze (GET) endpoint that starts code analysis and returns a streaming response. Checks for successful initiation (status 200, content-type 'text/event-stream'), the requirement of the repo_url parameter (status 400), and the initial fetching of commits and reading of files before starting the background task. Category: Functional Testing, Input Validation, Asynchronous Operation Testing (Simulated Start) Dependencies (Mocks): api.routes.analyze_repository_background , api.routes.get_commits_from_db , api.routes.read_files_from_dict_list ID: TC_ANALYSIS_STATUS_ENDPOINT Description: Verifies the functionality of the /analysis_status (GET) endpoint for retrieving the status of an analysis. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and the case where no status is found for the repo (status 404). Category: Functional Testing, Input Validation, State Verification Dependencies (Mocks): api.routes.get_analysis_status ID: TC_ANALYZEDB_ENDPOINT Description: Verifies the functionality of the /analyzedb (GET) endpoint for retrieving stored analysis results for a repo. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and handling cases where the database returns no data or raises an exception (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_analysis_from_db ID: TC_ANALYZEALL_ENDPOINT Description: Verifies the functionality of the /analyzeall (GET) endpoint for retrieving analysis results for all repositories. Checks for successful retrieval (status 200, data check) and handling cases where the database returns no data or raises an exception (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.get_allanalysis_from_db 6.2 Proposed New Test Cases These are ideas for additional unit tests that could improve coverage. ID: TC_LIST_COMMITS_INVALID_URL Description: Test the behavior of /commits (POST) when an invalid or inaccessible repo_url is provided. It is expected to return an appropriate error code (e.g., 500 or 400) and a clear error message, as clone_repo or pull_repo will likely fail. Category: Error Handling, Input Validation Difficulty: Medium (Requires setting up the mock to simulate a failed git command) Mock Example: mock_clone.side_effect = Exception(\"Git clone failed: repository not found\") ID: TC_LIST_COMMITS_LIMIT_PARAM Description: Test the limit parameter of /commits (POST). Try with limit=0 , limit=1 , and limit greater than the number of commits returned by the extract_contributions mock. Verify that the number of returned commits is as expected. Category: Functional Testing, Input Validation Difficulty: Easy Example: Add limit to the request JSON and check len(json.loads(response.data)) . ID: TC_CREATE_REPO_MISSING_FIELDS Description: Test the behavior of /repos (POST) when mandatory fields (if any, e.g., repo_name ) or optional fields are missing from the JSON payload. Verify that the endpoint either uses default values (as it seems to do now) or returns a 400 error if a field is strictly required. Category: Input Validation, Robustness Testing Difficulty: Easy Example: self.client.post('/repos', json={\"repo_name\": \"only_name\"}) ID: TC_EDIT_REPO_NOT_FOUND Description: Test the behavior of /repos/<repo_name> (PUT) when the repo_name provided in the URL does not correspond to an existing repository. The current implementation with ON CONFLICT DO UPDATE in save_repo_to_db would create a new repo. Consider if this is the desired behavior for an edit endpoint or if it should return 404. If it should return 404, the test must verify this (requires changes to the route logic or save_repo_to_db ). Category: Functional Testing, Edge Case Testing Difficulty: Medium (May reveal the need for changes in application logic) ID: TC_ANALYZE_NO_COMMITS Description: Test the behavior of /analyze (GET) when get_commits_from_db returns an empty list for the given repo_name . The endpoint is expected to immediately return an error (e.g., 400 or 404) with an appropriate message, instead of proceeding to read files or start the background task. Category: Error Handling, Edge Case Testing Difficulty: Easy Mock Example: mock_get_commits.return_value = [] ID: TC_ANALYZE_STREAM_ERROR Description: Test the stream of /analyze (GET) when analyze_repository_background produces an error during its execution (e.g., after sending some progress updates). Verify that the last message in the stream contains the error information. Category: Asynchronous Operation Testing, Error Handling Difficulty: Medium (Requires setting up the mock generator to yield data and then yield an error or raise Exception ) Mock Example: mock_analyze_background.return_value = iter([b'data: {\"progress\": 10}\\n\\n', b'data: {\"error\": \"Analysis failed\"}\\n\\n']) ID: TC_ANALYZE_STREAM_CONTENT Description: Test the content of the messages sent via the stream from /analyze (GET). For a successful scenario, verify that the JSON messages contain the expected fields ( progress , file_data or message , repoUrl ) and that the progress increases logically, eventually reaching 100%. Category: Functional Testing, Asynchronous Operation Testing, Data Validation Difficulty: Medium Example: Analyze the chunks returned from response.stream . ID: TC_INVALID_REPO_NAME_FORMAT Description: Test endpoints accepting repo_name as part of the URL (e.g., /repos/<repo_name> , /delete_repo/<repo_name> , /analysis_status?repo_name=... , /analyzedb?repo_name=... ) with invalid names (e.g., containing / , .. , special characters). Depending on how repo_name is used downstream (hopefully not directly in paths), they should either be rejected (400/404) or handled safely. Category: Input Validation, Security Testing (Basic) Difficulty: Easy/Medium","title":"Testing"},{"location":"components/ku-detection/tests/#testing-the-api-routes-component-apiroutespy","text":"","title":"Testing the API Routes Component (api/routes.py)"},{"location":"components/ku-detection/tests/#1-introduction","text":"This document describes the testing strategy and procedures for the Flask routes component located in api/routes.py . The goal is to ensure the correct operation, robustness, and expected behavior of the API endpoints. The primary methodology used is Unit Testing utilizing Python's built-in unittest and unittest.mock libraries.","title":"1. Introduction"},{"location":"components/ku-detection/tests/#2-testing-philosophy","text":"The tests for the routes ( api/test_routes.py ) are designed to be: Isolated: Each test focuses on a specific endpoint or operational scenario. External dependencies, primarily calls to the database ( api/data_db.py ) and time-consuming operations (like Git repo cloning or loading ML models), are bypassed (mocked) . Fast: Bypassing external dependencies allows for rapid test execution, facilitating integration into CI/CD pipelines and providing immediate feedback during development. Repeatable: Tests do not depend on the state of the actual database or file system and must yield the same result every time they are executed. Important: These unit tests do not interact with the actual PostgreSQL database . They use mocks to simulate database responses, ensuring the production database remains unaffected.","title":"2. Testing Philosophy"},{"location":"components/ku-detection/tests/#21-testing-strategy","text":"For the API routes component ( api/routes.py ), the following types of testing are applied: Unit Testing: (Primary Method) Testing each API endpoint in isolation, using mocks to isolate from external dependencies (database, Git, ML model). Focuses on the internal logic of the route, request/response handling, and calling the appropriate (mocked) external functions. Functional Testing: Verifying that the endpoints behave as expected based on specifications for specific inputs and usage scenarios. Input Validation Testing: Checking the behavior of endpoints when receiving valid, invalid, boundary, or incomplete input data (URL parameters, query parameters, JSON payloads). Error Handling Testing: Ensuring the application correctly handles errors (e.g., database failure, invalid repository URL, analysis errors) and returns appropriate HTTP status codes and error messages. Asynchronous Operation Testing (Simulated): Testing the initiation and response structure (e.g., Server-Sent Events for /analyze ) of background operations by simulating their behavior with mocks. (Potential future types of testing, complementary to the existing unit tests, could include): Integration Testing (Database): Description: Testing the interaction of API routes with a real (but separate, for testing) PostgreSQL database. This would verify that data is written and read correctly, database constraints work as expected, and more complex queries (if any) return correct results. Goal: Ensure the application functions correctly in conjunction with the actual database system. Integration Testing (Git Operations): Description: Testing the Git-interacting functions ( clone_repo , pull_repo , extract_contributions , get_history_repo ) by executing real Git commands against one or more real (perhaps temporary or sample) Git repositories. Goal: Verify the application can correctly handle Git repositories, including cases like clone failure (e.g., wrong URL, private repo), history changes, etc. Integration Testing (ML Model Analysis): Description: Testing the full analysis flow ( /analyze endpoint and codebert_sliding_window ) by loading and using the actual CodeBERT model . Could check if results are produced (without necessarily judging the correctness of KUs) for sample code and if errors during model loading or execution are handled gracefully. Goal: Ensure the analysis system works end-to-end with the real ML model. These tests might be slow. End-to-End (E2E) Testing: Description: Simulating complete user scenarios from the perspective of an external client calling the API. For example: a) Add repo, b) Fetch commits, c) Start analysis, d) Check status, e) Retrieve results. Goal: Verify that the different components (routes, database, git operations, analysis) cooperate correctly to complete a full workflow. Load / Performance Testing: Description: Using tools (e.g., Locust, k6, JMeter) to simulate multiple concurrent users calling the API endpoints, especially demanding ones like /analyze and /commits . Measure response times, error rates, and resource usage (CPU, memory, database connections) under load. Goal: Identify bottlenecks, evaluate application scalability, and ensure acceptable performance under expected load. Concurrency Testing: Description: Targeted testing of scenarios where multiple operations run simultaneously, especially around the background analysis ( /analyze ). Check for potential race conditions (e.g., updating analysis status in the database from multiple threads), deadlocks, or resource exhaustion (e.g., excessive number of threads). Goal: Ensure application stability and correctness when multiple analyses run concurrently.","title":"2.1 Testing Strategy"},{"location":"components/ku-detection/tests/#3-running-the-unit-tests","text":"To run the specific unit tests for the API routes: Ensure you have installed the necessary dependencies (usually via pip install -r requirements.txt in the project's root directory) and activated your virtual environment. Execute the following command from the project's root directory: bash python -m unittest api.test_routes.py","title":"3. Running the Unit Tests"},{"location":"components/ku-detection/tests/#4-existing-test-suite-overview","text":"The current tests in api/test_routes.py cover the basic functionality of the following endpoints: /commits (POST): Retrieve commits, handle clone/pull. /repos (POST): Create a new repository entry. /detected_kus (GET): Retrieve detected KUs. /repos/<repo_name> (PUT): Edit an existing repository entry. /timestamps (GET): Retrieve commit timestamps. /historytime (GET): Retrieve commit history timeline. /delete_repo/<repo_name> (DELETE): Delete a repository entry. /repos (GET): List all repository entries. /analyze (GET): Start the analysis process (streaming response). /analysis_status (GET): Retrieve analysis status. /analyzedb (GET): Retrieve stored analysis results for a repo. /analyzeall (GET): Retrieve all stored analysis results. A detailed description of each test case is provided in the Test Case Catalog below.","title":"4. Existing Test Suite Overview"},{"location":"components/ku-detection/tests/#5-adding-new-unit-tests","text":"When adding new endpoints or modifying existing ones in api/routes.py , corresponding unit tests should also be added or updated in api/test_routes.py . Follow these steps: Create Test Function: Add a new method to the FlaskAPITests class that starts with test_ (e.g., test_new_endpoint_success ). Use @patch : Use the @patch decorator to mock all external dependencies called by your route (e.g., functions from api.data_db , core.git_operations , etc.). Target the function where it is used (usually imported into api.routes ). The mocks are passed as arguments to your test function (in reverse order of the decorators). python @patch('api.routes.some_db_function') @patch('api.routes.another_external_call') def test_new_endpoint_success(self, mock_external_call, mock_db_function): # Mocks are passed in reverse order: # mock_external_call corresponds to @patch('api.routes.another_external_call') # mock_db_function corresponds to @patch('api.routes.some_db_function') # ... Configure Mocks: Inside your test function, set the return values ( return_value ) or side effects ( side_effect ) of the mock objects to simulate various scenarios (e.g., successful data retrieval, database error, empty results). python mock_db_function.return_value = [{'id': 1, 'data': 'sample'}] mock_external_call.side_effect = Exception(\"Network Error\") Call Endpoint: Use self.client (the Flask test client, available in FlaskAPITests ) to send the appropriate HTTP request to the endpoint you want to test. python response = self.client.get('/new_endpoint?param=value') response = self.client.post('/another_endpoint', json={'key': 'value'}) Assertions: Use unittest 's assert methods (e.g., self.assertEqual , self.assertTrue , self.assertIn ) to verify: The response status code: self.assertEqual(response.status_code, 200) The content type: self.assertEqual(response.content_type, 'application/json') The response data: data = json.loads(response.data); self.assertEqual(data['message'], 'Success') Whether the mocked functions were called correctly: mock_db_function.assert_called_once_with(...) or mock_external_call.assert_called_once()","title":"5. Adding New Unit Tests"},{"location":"components/ku-detection/tests/#6-test-case-catalog","text":"This catalog details the existing and proposed unit tests for the API routes.","title":"6. Test Case Catalog"},{"location":"components/ku-detection/tests/#61-existing-test-cases-apitest_routespy","text":"ID: TC_LIST_COMMITS Description: Verifies the functionality of the /commits (POST) endpoint for retrieving a list of commits from a repository. Tests two scenarios: a) when the repo does not exist locally (requires clone_repo ), and b) when the repo exists (requires pull_repo ). Verifies the status code (200) and the correctness of the returned data. Also checks that the appropriate functions (clone/pull, extract, save) are called. Category: Functional Testing, Integration Testing (Simulated - Git Operations & DB) Dependencies (Mocks): api.routes.save_commits_to_db , api.routes.extract_contributions , api.routes.pull_repo , api.routes.repo_exists , api.routes.clone_repo ID: TC_CREATE_REPO Description: Verifies the functionality of the /repos (POST) endpoint for creating a new repository entry. Checks for successful creation (status 201, success message) and proper error handling during database saving (status 500, error message). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.save_repo_to_db ID: TC_GET_DETECTED_KUS Description: Verifies the functionality of the /detected_kus (GET) endpoint for retrieving the list of detected knowledge units. Checks for successful retrieval (status 200, data check), the case where the database returns no data (status 500), and handling of database exceptions (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.getdetected_kus ID: TC_EDIT_REPO Description: Verifies the functionality of the /repos/<repo_name> (PUT) endpoint for updating information of an existing repository. Checks for successful update (status 200, success message) and error handling during database saving (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.save_repo_to_db ID: TC_GET_TIMESTAMPS Description: Verifies the functionality of the /timestamps (GET) endpoint for retrieving commit timestamps for a specific repository. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and handling cases where the database returns no data (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_commits_timestamps_from_db ID: TC_HISTORYTIME Description: Verifies the functionality of the /historytime (GET) endpoint for retrieving the timeline of commit dates for a repository. Checks for successful retrieval (status 200, structure and data check), the requirement of the repo_url parameter (status 400), and handling exceptions during history retrieval (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_history_repo ID: TC_DELETE_REPO Description: Verifies the functionality of the /delete_repo/<repo_name> (DELETE) endpoint for deleting a repository entry. Checks for successful deletion (status 200, success message) and handling exceptions during database deletion (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.delete_repo_from_db ID: TC_LIST_REPOS Description: Verifies the functionality of the /repos (GET) endpoint for retrieving the list of all repositories. Checks for successful retrieval (status 200, structure and data check) and handling of database exceptions (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.get_all_repos_from_db ID: TC_ANALYZE_ENDPOINT Description: Verifies the functionality of the /analyze (GET) endpoint that starts code analysis and returns a streaming response. Checks for successful initiation (status 200, content-type 'text/event-stream'), the requirement of the repo_url parameter (status 400), and the initial fetching of commits and reading of files before starting the background task. Category: Functional Testing, Input Validation, Asynchronous Operation Testing (Simulated Start) Dependencies (Mocks): api.routes.analyze_repository_background , api.routes.get_commits_from_db , api.routes.read_files_from_dict_list ID: TC_ANALYSIS_STATUS_ENDPOINT Description: Verifies the functionality of the /analysis_status (GET) endpoint for retrieving the status of an analysis. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and the case where no status is found for the repo (status 404). Category: Functional Testing, Input Validation, State Verification Dependencies (Mocks): api.routes.get_analysis_status ID: TC_ANALYZEDB_ENDPOINT Description: Verifies the functionality of the /analyzedb (GET) endpoint for retrieving stored analysis results for a repo. Checks for successful retrieval (status 200, data check), the requirement of the repo_name parameter (status 400), and handling cases where the database returns no data or raises an exception (status 500). Category: Functional Testing, Input Validation, Error Handling Dependencies (Mocks): api.routes.get_analysis_from_db ID: TC_ANALYZEALL_ENDPOINT Description: Verifies the functionality of the /analyzeall (GET) endpoint for retrieving analysis results for all repositories. Checks for successful retrieval (status 200, data check) and handling cases where the database returns no data or raises an exception (status 500). Category: Functional Testing, Error Handling Dependencies (Mocks): api.routes.get_allanalysis_from_db","title":"6.1 Existing Test Cases (api/test_routes.py)"},{"location":"components/ku-detection/tests/#62-proposed-new-test-cases","text":"These are ideas for additional unit tests that could improve coverage. ID: TC_LIST_COMMITS_INVALID_URL Description: Test the behavior of /commits (POST) when an invalid or inaccessible repo_url is provided. It is expected to return an appropriate error code (e.g., 500 or 400) and a clear error message, as clone_repo or pull_repo will likely fail. Category: Error Handling, Input Validation Difficulty: Medium (Requires setting up the mock to simulate a failed git command) Mock Example: mock_clone.side_effect = Exception(\"Git clone failed: repository not found\") ID: TC_LIST_COMMITS_LIMIT_PARAM Description: Test the limit parameter of /commits (POST). Try with limit=0 , limit=1 , and limit greater than the number of commits returned by the extract_contributions mock. Verify that the number of returned commits is as expected. Category: Functional Testing, Input Validation Difficulty: Easy Example: Add limit to the request JSON and check len(json.loads(response.data)) . ID: TC_CREATE_REPO_MISSING_FIELDS Description: Test the behavior of /repos (POST) when mandatory fields (if any, e.g., repo_name ) or optional fields are missing from the JSON payload. Verify that the endpoint either uses default values (as it seems to do now) or returns a 400 error if a field is strictly required. Category: Input Validation, Robustness Testing Difficulty: Easy Example: self.client.post('/repos', json={\"repo_name\": \"only_name\"}) ID: TC_EDIT_REPO_NOT_FOUND Description: Test the behavior of /repos/<repo_name> (PUT) when the repo_name provided in the URL does not correspond to an existing repository. The current implementation with ON CONFLICT DO UPDATE in save_repo_to_db would create a new repo. Consider if this is the desired behavior for an edit endpoint or if it should return 404. If it should return 404, the test must verify this (requires changes to the route logic or save_repo_to_db ). Category: Functional Testing, Edge Case Testing Difficulty: Medium (May reveal the need for changes in application logic) ID: TC_ANALYZE_NO_COMMITS Description: Test the behavior of /analyze (GET) when get_commits_from_db returns an empty list for the given repo_name . The endpoint is expected to immediately return an error (e.g., 400 or 404) with an appropriate message, instead of proceeding to read files or start the background task. Category: Error Handling, Edge Case Testing Difficulty: Easy Mock Example: mock_get_commits.return_value = [] ID: TC_ANALYZE_STREAM_ERROR Description: Test the stream of /analyze (GET) when analyze_repository_background produces an error during its execution (e.g., after sending some progress updates). Verify that the last message in the stream contains the error information. Category: Asynchronous Operation Testing, Error Handling Difficulty: Medium (Requires setting up the mock generator to yield data and then yield an error or raise Exception ) Mock Example: mock_analyze_background.return_value = iter([b'data: {\"progress\": 10}\\n\\n', b'data: {\"error\": \"Analysis failed\"}\\n\\n']) ID: TC_ANALYZE_STREAM_CONTENT Description: Test the content of the messages sent via the stream from /analyze (GET). For a successful scenario, verify that the JSON messages contain the expected fields ( progress , file_data or message , repoUrl ) and that the progress increases logically, eventually reaching 100%. Category: Functional Testing, Asynchronous Operation Testing, Data Validation Difficulty: Medium Example: Analyze the chunks returned from response.stream . ID: TC_INVALID_REPO_NAME_FORMAT Description: Test endpoints accepting repo_name as part of the URL (e.g., /repos/<repo_name> , /delete_repo/<repo_name> , /analysis_status?repo_name=... , /analyzedb?repo_name=... ) with invalid names (e.g., containing / , .. , special characters). Depending on how repo_name is used downstream (hopefully not directly in paths), they should either be rejected (400/404) or handled safely. Category: Input Validation, Security Testing (Basic) Difficulty: Easy/Medium","title":"6.2 Proposed New Test Cases"},{"location":"components/universities-curriculum/","text":"\ud83e\udde0 SKILLCRAWL SkillCrawl is a tool that helps you search into university curricula and extract important skills that students are expected to learn \u2014 all from PDF files. It uses the official ESCO skill framework and leverages the esco-skill-extractor to identify real-world competencies and link them back to course content. You can run it interactively in your terminal, use its web API with Swagger UI, or even connect it to a MySQL database. Whether you're doing academic research, analyzing skill trends, or building a course recommender, SkillCrawl saves you time by automating the skill detection. \u2699\ufe0f Coming soon : An automated web crawler is currently under development to let you scan entire university websites for curriculum pages \u2014 no PDFs required! It\u2019s still experimental but already crawling basic lesson data from real websites. \ud83d\ude80 What Does SkillCrawl Actually Do? SkillCrawl is like a skill-focused academic detective. Here's how it works: \ud83d\udce5 Takes in PDF files of university curricula. \ud83d\udd0d Reads and processes the text using PDF-aware techniques. \ud83d\uddc2\ufe0f Splits course content into semesters and individual lessons , even if the formatting is inconsistent. \ud83e\udde9 For each lesson, it extracts related ESCO skills (the EU\u2019s official database of competencies and qualifications). \ud83e\udde0 Lets you search by skill (e.g., \"data analysis\") and find which courses teach it \u2014 or browse what skills each course covers. \ud83d\udee2\ufe0f Supports saving everything to MySQL , so you can build dashboards or analyze data later. \ud83d\udd01 Keeps a local cache of all processed files for instant reuse. \ud83d\udcac Comes with both a full FastAPI/Swagger web interface and a terminal menu . \ud83c\udf10 (Experimental) A crawler module is being built to auto-visit university websites and scrape curriculum content without any manual download. \ud83d\udee0\ufe0f Usage \u27a4 FastAPI Mode (Swagger UI) Run the API with: uvicorn main:app --reload Then open: http://127.0.0.1:8000/docs#/ Here you can: - Process PDFs - Search for skills (by name or URL) or courses -in a specific university or globally- - Get top N skills per university or globally - Fetch full lesson/skill breakdowns - Save data to DB with a click Ensure: - Your database ( SkillCrawl ) is set up using skillcrawl.sql . - XAMPP/MySQL is running. \ud83e\uddea Example Output SkillCrawl prints matching lessons and their associated ESCO skills: Semester 1: Introduction to Programming Skill: problem solving Skill: code debugging Or search in reverse: Skill: machine learning Matched Course: AI and Ethics (Score: 84) \ud83c\udfd7\ufe0f Project Structure File/Folder Purpose main.py FastAPI app skillcrawl.py Entry point with menu & CLI control pdf_utils.py PDF parsing, semester/lesson splitting database.py Handles writing to MySQL skills.py Skill extraction, DB lookup, and fuzzy matching output.py Visual output helpers (ASCII, color, lines) config.py DB and cache config helpers.py Utility functions (caching, validation, detection) menu.py Terminal ASCII Interface skillcrawl.sql SQL schema to set up the database requirements.txt List of required Python libraries cache/ Stores processed data by university tests/ Stores the available test cases you can run README.md You're reading it! \ud83d\udcda Use Cases Curriculum Analysis & Benchmarking Educational Skill Mapping Automated Course Tagging Career Pathway Recommendations Academic Skill Graph Construction \ud83e\udde9 Requirements Install all dependencies: pip install -r requirements.txt Recommended: Python 3.10 Avoid Python 3.11+ due to compatibility with esco-skill-extractor . \ud83d\udcbe Database Setup Initialize the database using: mysql -u root -p < skillcrawl.sql Make sure to configure the connection details in skillcrawl.py and skills.py under db_config . \ud83d\udd04 Caching Processed results are cached per university PDF in: - cache/pdf_cache.json - university_cache.json Where university is replaced by a respective university name that the cache represents. You can delete these files to force re-processing. \ud83d\udd0d Key Method Explanations Here are the most important methods across the codebase explained for better understanding and easier contribution: main.py (FastAPI) /process_pdf : Endpoint to process a PDF, extract text, split lessons, run skill extraction, and cache results. /search_skill : Search database for lessons teaching a given skill. /calculate_skillnames : Enriches lessons with missing skill names via Skillab Tracker API. /get_top_skills & /get_top_skills_all : Return most frequently taught skills globally or per university. /filter_skillnames : Lookup skill names for a specific university and lesson using either DB or cache. skillcrawl.py main(...) : The main method that runs the terminal interface. Handles PDF processing, caching, and triggering the desired output based on command-line args. get_university_country(university_name) : Uses an external API to detect a university's country from its name. Updates local cache. pdf_utils.py extract_text_from_pdf(pdf_file_path) : Uses PyMuPDF to extract text from each page of the PDF. Also caches results. extract_text_after_marker(text, markers) : Takes all text and returns everything after specific marker words like \"Course Content\". split_by_semester(text) : Breaks the PDF text into sections by semester or year using regex. process_pages_by_lesson(pages) : Processes PDF page-by-page to detect lessons and their descriptions using uppercase pattern recognition. skills.py get_skills_for_lesson(...) : Looks up all skills associated with a given lesson or university. Optionally searches by lesson name. extract_and_get_title(skill_url) : Fetches the readable name of a skill from its ESCO URL using their API. search_courses_by_skill_database(...) : Searches all courses for a fuzzy match of the given skill name in the database. database.py write_to_database(...) : Saves extracted data to MySQL: university, semesters, lessons, and skills. Also merges new skills with what's already in the cache. helpers.py find_possible_university(pdf_file_path) : Scans PDF content using regex to guess the university name. load_from_cache(university_name) / save_to_cache(...) : Manages university-specific cache JSONs. contains_greek_characters(...) / contains_no_lowercase_letters(...) : Utilities used in filtering invalid lesson names. extract_description(text) : Strips out and formats a clean description from lesson blocks.","title":"General Info"},{"location":"components/universities-curriculum/#skillcrawl","text":"SkillCrawl is a tool that helps you search into university curricula and extract important skills that students are expected to learn \u2014 all from PDF files. It uses the official ESCO skill framework and leverages the esco-skill-extractor to identify real-world competencies and link them back to course content. You can run it interactively in your terminal, use its web API with Swagger UI, or even connect it to a MySQL database. Whether you're doing academic research, analyzing skill trends, or building a course recommender, SkillCrawl saves you time by automating the skill detection. \u2699\ufe0f Coming soon : An automated web crawler is currently under development to let you scan entire university websites for curriculum pages \u2014 no PDFs required! It\u2019s still experimental but already crawling basic lesson data from real websites.","title":"\ud83e\udde0 SKILLCRAWL"},{"location":"components/universities-curriculum/#what-does-skillcrawl-actually-do","text":"SkillCrawl is like a skill-focused academic detective. Here's how it works: \ud83d\udce5 Takes in PDF files of university curricula. \ud83d\udd0d Reads and processes the text using PDF-aware techniques. \ud83d\uddc2\ufe0f Splits course content into semesters and individual lessons , even if the formatting is inconsistent. \ud83e\udde9 For each lesson, it extracts related ESCO skills (the EU\u2019s official database of competencies and qualifications). \ud83e\udde0 Lets you search by skill (e.g., \"data analysis\") and find which courses teach it \u2014 or browse what skills each course covers. \ud83d\udee2\ufe0f Supports saving everything to MySQL , so you can build dashboards or analyze data later. \ud83d\udd01 Keeps a local cache of all processed files for instant reuse. \ud83d\udcac Comes with both a full FastAPI/Swagger web interface and a terminal menu . \ud83c\udf10 (Experimental) A crawler module is being built to auto-visit university websites and scrape curriculum content without any manual download.","title":"\ud83d\ude80 What Does SkillCrawl Actually Do?"},{"location":"components/universities-curriculum/#usage","text":"","title":"\ud83d\udee0\ufe0f Usage"},{"location":"components/universities-curriculum/#fastapi-mode-swagger-ui","text":"Run the API with: uvicorn main:app --reload Then open: http://127.0.0.1:8000/docs#/ Here you can: - Process PDFs - Search for skills (by name or URL) or courses -in a specific university or globally- - Get top N skills per university or globally - Fetch full lesson/skill breakdowns - Save data to DB with a click Ensure: - Your database ( SkillCrawl ) is set up using skillcrawl.sql . - XAMPP/MySQL is running.","title":"\u27a4 FastAPI Mode (Swagger UI)"},{"location":"components/universities-curriculum/#example-output","text":"SkillCrawl prints matching lessons and their associated ESCO skills: Semester 1: Introduction to Programming Skill: problem solving Skill: code debugging Or search in reverse: Skill: machine learning Matched Course: AI and Ethics (Score: 84)","title":"\ud83e\uddea Example Output"},{"location":"components/universities-curriculum/#project-structure","text":"File/Folder Purpose main.py FastAPI app skillcrawl.py Entry point with menu & CLI control pdf_utils.py PDF parsing, semester/lesson splitting database.py Handles writing to MySQL skills.py Skill extraction, DB lookup, and fuzzy matching output.py Visual output helpers (ASCII, color, lines) config.py DB and cache config helpers.py Utility functions (caching, validation, detection) menu.py Terminal ASCII Interface skillcrawl.sql SQL schema to set up the database requirements.txt List of required Python libraries cache/ Stores processed data by university tests/ Stores the available test cases you can run README.md You're reading it!","title":"\ud83c\udfd7\ufe0f Project Structure"},{"location":"components/universities-curriculum/#use-cases","text":"Curriculum Analysis & Benchmarking Educational Skill Mapping Automated Course Tagging Career Pathway Recommendations Academic Skill Graph Construction","title":"\ud83d\udcda Use Cases"},{"location":"components/universities-curriculum/#requirements","text":"Install all dependencies: pip install -r requirements.txt Recommended: Python 3.10 Avoid Python 3.11+ due to compatibility with esco-skill-extractor .","title":"\ud83e\udde9 Requirements"},{"location":"components/universities-curriculum/#database-setup","text":"Initialize the database using: mysql -u root -p < skillcrawl.sql Make sure to configure the connection details in skillcrawl.py and skills.py under db_config .","title":"\ud83d\udcbe Database Setup"},{"location":"components/universities-curriculum/#caching","text":"Processed results are cached per university PDF in: - cache/pdf_cache.json - university_cache.json Where university is replaced by a respective university name that the cache represents. You can delete these files to force re-processing.","title":"\ud83d\udd04 Caching"},{"location":"components/universities-curriculum/#key-method-explanations","text":"Here are the most important methods across the codebase explained for better understanding and easier contribution:","title":"\ud83d\udd0d Key Method Explanations"},{"location":"components/universities-curriculum/#mainpy-fastapi","text":"/process_pdf : Endpoint to process a PDF, extract text, split lessons, run skill extraction, and cache results. /search_skill : Search database for lessons teaching a given skill. /calculate_skillnames : Enriches lessons with missing skill names via Skillab Tracker API. /get_top_skills & /get_top_skills_all : Return most frequently taught skills globally or per university. /filter_skillnames : Lookup skill names for a specific university and lesson using either DB or cache.","title":"main.py (FastAPI)"},{"location":"components/universities-curriculum/#skillcrawlpy","text":"main(...) : The main method that runs the terminal interface. Handles PDF processing, caching, and triggering the desired output based on command-line args. get_university_country(university_name) : Uses an external API to detect a university's country from its name. Updates local cache.","title":"skillcrawl.py"},{"location":"components/universities-curriculum/#pdf_utilspy","text":"extract_text_from_pdf(pdf_file_path) : Uses PyMuPDF to extract text from each page of the PDF. Also caches results. extract_text_after_marker(text, markers) : Takes all text and returns everything after specific marker words like \"Course Content\". split_by_semester(text) : Breaks the PDF text into sections by semester or year using regex. process_pages_by_lesson(pages) : Processes PDF page-by-page to detect lessons and their descriptions using uppercase pattern recognition.","title":"pdf_utils.py"},{"location":"components/universities-curriculum/#skillspy","text":"get_skills_for_lesson(...) : Looks up all skills associated with a given lesson or university. Optionally searches by lesson name. extract_and_get_title(skill_url) : Fetches the readable name of a skill from its ESCO URL using their API. search_courses_by_skill_database(...) : Searches all courses for a fuzzy match of the given skill name in the database.","title":"skills.py"},{"location":"components/universities-curriculum/#databasepy","text":"write_to_database(...) : Saves extracted data to MySQL: university, semesters, lessons, and skills. Also merges new skills with what's already in the cache.","title":"database.py"},{"location":"components/universities-curriculum/#helperspy","text":"find_possible_university(pdf_file_path) : Scans PDF content using regex to guess the university name. load_from_cache(university_name) / save_to_cache(...) : Manages university-specific cache JSONs. contains_greek_characters(...) / contains_no_lowercase_letters(...) : Utilities used in filtering invalid lesson names. extract_description(text) : Strips out and formats a clean description from lesson blocks.","title":"helpers.py"},{"location":"components/universities-curriculum/testing/","text":"\ud83e\uddea SkillCrawl Unit Test Suite This test suite ensures the quality of SkillCrawl\u2019s outputs across multiple dimensions such as correctness, integrity, and completeness. Tests are categorized into the following quality dimensions: Category Goal \u2705 Data Integrity No duplicates or corrupted entries \ud83d\udcd0 Consistency Stable, explainable patterns in results \ud83d\udcca Completeness No missing or under-analyzed lessons \ud83c\udfaf Accuracy Extracted skills match expected outcomes from trusted datasets \ud83d\udcc1 Structure tests/ \u251c\u2500\u2500 json/ \u2502 \u2514\u2500\u2500 [Expected output JSON files] \u251c\u2500\u2500 sample_pdfs/ \u2502 \u2514\u2500\u2500 test_curriculum.pdf \u251c\u2500\u2500 test_compare_extracted_titles.py \u251c\u2500\u2500 test_compare_extracted_skills.py \u251c\u2500\u2500 test_compare_top_skills.py \u251c\u2500\u2500 test_compare_skill_search.py \u2514\u2500\u2500 ... \u2705 Existing Test Cases \ud83c\udfaf Accuracy Testing 1. Compare Extracted Titles from PDF File: test_compare_extracted_titles.py Compares: Method output \u2192 extracted_titles_expected.json Difficulty: \ud83d\udfe9 Easy Purpose: Extracts titles from a sample PDF using process_pdf() and checks them against the confirmed set. Method Called: process_pdf(PDFProcessingRequest) Expected File: json/extracted_titles_expected.json \ud83d\udd0d Compares returned lesson titles (per semester) directly to the expected JSON. \u2714\ufe0f Validates that all course titles are correctly extracted per semester. 2. Skill Extraction from Course Descriptions File: test_compare_extracted_skills.py Compares: Method output \u2192 extracted_skills_expected.json Difficulty: \ud83d\udfe8 Medium Purpose: Uses process_pdf() and calculate_skillnames() to extract skills from course descriptions and compare to expected. Methods Called: process_pdf(PDFProcessingRequest) calculate_skillnames(university_name) Expected File: json/extracted_skills_expected.json \ud83e\udde0 Confirms that all expected skills per lesson are detected, including names. \u2714\ufe0f Ensures that skills extracted from lesson descriptions match confirmed ones. 3. Top N Skills for a University File: test_compare_top_skills.py Compares: Method output \u2192 top_skills_cambridge_expected.json Difficulty: \ud83d\udfe9 Easy Purpose: Verifies that get_top_skills() returns the correct top N skills for a university with the expected frequency. Method Called: get_top_skills(TopSkillsRequest) Expected File: json/top_skills_cambridge_expected.json \ud83d\udcca Checks that skill names and counts match expected results. \u2714\ufe0f Checks top ESCO skills returned by the system and their frequency. 4. Skill-based Search Results File: test_compare_skill_search.py Method: search_skill() Difficulty: \ud83d\udfe8 Medium Purpose: Runs a live skill-based course search and compares the results to verified course/skill/university matches. Method Called: search_skill(SkillSearchRequest) Expected File: json/C_skill_search_expected.json \ud83d\udd0e Ensures that returned results match the expected structure and content, including frequency and fuzzy score. \u2714\ufe0f Verifies courses matched to a given skill (across all universities). \ud83e\uddea Proposed New Tests (for expansion) These are not yet implemented purposefully \u2014 they are ideal for a challenge at the testathon! \u2705 Data Integrity Testing 5. Duplicate Skills Per Lesson Goal: Ensure no skill is assigned more than once to the same lesson. Method(s): Check calculate_skillnames() output Difficulty: \ud83d\udfe9 Easy \u2714\ufe0f Loop through all lessons and ensure all skill names are unique. 6. Duplicate Skills Across University Goal: Detect redundant skill entries across multiple lessons in the same university. Method(s): Use get_skills_for_lesson() Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Count how often each skill appears and detect unnecessary repetition. \ud83d\udcd0 Consistency Testing 7. Cross-Course Skill Similarity Goal: Ensure that related lessons (e.g., under the same department or semester) share logically consistent skill sets. Method(s): Compare lesson skill sets using similarity scoring. Difficulty: \ud83d\udfe5 Hard \u2714\ufe0f Flag anomalies where unrelated skill sets appear within similar contexts. 8. Inconsistent Skill Distribution Goal: Detect universities where similar lesson names have completely disjoint skills. Method(s): Cluster lessons and analyze skill overlap. Difficulty: \ud83d\udfe5 Hard \u2714\ufe0f Suggest possible errors or inconsistencies in extraction. \ud83d\udcca Completeness Testing 9. Missing Skills in Lessons Goal: Flag any lessons with zero skills after processing. Method(s): calculate_skillnames() Difficulty: \ud83d\udfe9 Easy \u2714\ufe0f Useful to catch bad input data or failed OCR. 10. Minimum Skill Threshold Goal: Validate that each lesson has at least N skills (e.g., 3). Method(s): Compare count from skill output. Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Helps surface under-analyzed lessons. \ud83c\udfaf Accuracy Testing (Advanced) 11. Skill Mapping Validation with ESCO Labels Goal: Compare actual ESCO preferred labels with what was extracted. Method(s): Use extract_and_get_title() Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Detect mismatches between ESCO URLs and their resolved names. 12. API JSON Schema Validation Goal: Ensure the API responses match expected structure and required fields. Method(s): FastAPI schema + pydantic Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Useful for future-proofing API integrations. \u25b6\ufe0f How to Run Tests Install requirements: pip install -r requirements.txt Run all tests: pytest tests/ Run a specific test: pytest tests/test_compare_extracted_skills.py \ud83e\udde0 Summary Table Test Case Type Difficulty Title extraction \ud83c\udfaf Accuracy \ud83d\udfe9 Easy Skill extraction from descriptions \ud83c\udfaf Accuracy \ud83d\udfe8 Medium Top skills per university \ud83c\udfaf Accuracy \ud83d\udfe9 Easy Skill-based search \ud83c\udfaf Accuracy \ud83d\udfe8 Medium Detect duplicate skills \u2705 Data Integrity \ud83d\udfe9 Easy Duplicate across university \u2705 Data Integrity \ud83d\udfe8 Medium Cross-course similarity \ud83d\udcd0 Consistency \ud83d\udfe5 Hard Inconsistent skill distribution \ud83d\udcd0 Consistency \ud83d\udfe5 Hard Missing skills \ud83d\udcca Completeness \ud83d\udfe9 Easy Minimum skills per lesson \ud83d\udcca Completeness \ud83d\udfe8 Medium ESCO label mismatch \ud83c\udfaf Accuracy \ud83d\udfe8 Medium API schema check \ud83c\udfaf Accuracy \ud83d\udfe8 Medium","title":"Testing"},{"location":"components/universities-curriculum/testing/#skillcrawl-unit-test-suite","text":"This test suite ensures the quality of SkillCrawl\u2019s outputs across multiple dimensions such as correctness, integrity, and completeness. Tests are categorized into the following quality dimensions: Category Goal \u2705 Data Integrity No duplicates or corrupted entries \ud83d\udcd0 Consistency Stable, explainable patterns in results \ud83d\udcca Completeness No missing or under-analyzed lessons \ud83c\udfaf Accuracy Extracted skills match expected outcomes from trusted datasets","title":"\ud83e\uddea SkillCrawl Unit Test Suite"},{"location":"components/universities-curriculum/testing/#structure","text":"tests/ \u251c\u2500\u2500 json/ \u2502 \u2514\u2500\u2500 [Expected output JSON files] \u251c\u2500\u2500 sample_pdfs/ \u2502 \u2514\u2500\u2500 test_curriculum.pdf \u251c\u2500\u2500 test_compare_extracted_titles.py \u251c\u2500\u2500 test_compare_extracted_skills.py \u251c\u2500\u2500 test_compare_top_skills.py \u251c\u2500\u2500 test_compare_skill_search.py \u2514\u2500\u2500 ...","title":"\ud83d\udcc1 Structure"},{"location":"components/universities-curriculum/testing/#existing-test-cases","text":"","title":"\u2705 Existing Test Cases"},{"location":"components/universities-curriculum/testing/#accuracy-testing","text":"","title":"\ud83c\udfaf Accuracy Testing"},{"location":"components/universities-curriculum/testing/#1-compare-extracted-titles-from-pdf","text":"File: test_compare_extracted_titles.py Compares: Method output \u2192 extracted_titles_expected.json Difficulty: \ud83d\udfe9 Easy Purpose: Extracts titles from a sample PDF using process_pdf() and checks them against the confirmed set. Method Called: process_pdf(PDFProcessingRequest) Expected File: json/extracted_titles_expected.json \ud83d\udd0d Compares returned lesson titles (per semester) directly to the expected JSON. \u2714\ufe0f Validates that all course titles are correctly extracted per semester.","title":"1. Compare Extracted Titles from PDF"},{"location":"components/universities-curriculum/testing/#2-skill-extraction-from-course-descriptions","text":"File: test_compare_extracted_skills.py Compares: Method output \u2192 extracted_skills_expected.json Difficulty: \ud83d\udfe8 Medium Purpose: Uses process_pdf() and calculate_skillnames() to extract skills from course descriptions and compare to expected. Methods Called: process_pdf(PDFProcessingRequest) calculate_skillnames(university_name) Expected File: json/extracted_skills_expected.json \ud83e\udde0 Confirms that all expected skills per lesson are detected, including names. \u2714\ufe0f Ensures that skills extracted from lesson descriptions match confirmed ones.","title":"2. Skill Extraction from Course Descriptions"},{"location":"components/universities-curriculum/testing/#3-top-n-skills-for-a-university","text":"File: test_compare_top_skills.py Compares: Method output \u2192 top_skills_cambridge_expected.json Difficulty: \ud83d\udfe9 Easy Purpose: Verifies that get_top_skills() returns the correct top N skills for a university with the expected frequency. Method Called: get_top_skills(TopSkillsRequest) Expected File: json/top_skills_cambridge_expected.json \ud83d\udcca Checks that skill names and counts match expected results. \u2714\ufe0f Checks top ESCO skills returned by the system and their frequency.","title":"3. Top N Skills for a University"},{"location":"components/universities-curriculum/testing/#4-skill-based-search-results","text":"File: test_compare_skill_search.py Method: search_skill() Difficulty: \ud83d\udfe8 Medium Purpose: Runs a live skill-based course search and compares the results to verified course/skill/university matches. Method Called: search_skill(SkillSearchRequest) Expected File: json/C_skill_search_expected.json \ud83d\udd0e Ensures that returned results match the expected structure and content, including frequency and fuzzy score. \u2714\ufe0f Verifies courses matched to a given skill (across all universities).","title":"4. Skill-based Search Results"},{"location":"components/universities-curriculum/testing/#proposed-new-tests-for-expansion","text":"These are not yet implemented purposefully \u2014 they are ideal for a challenge at the testathon!","title":"\ud83e\uddea Proposed New Tests (for expansion)"},{"location":"components/universities-curriculum/testing/#data-integrity-testing","text":"","title":"\u2705 Data Integrity Testing"},{"location":"components/universities-curriculum/testing/#5-duplicate-skills-per-lesson","text":"Goal: Ensure no skill is assigned more than once to the same lesson. Method(s): Check calculate_skillnames() output Difficulty: \ud83d\udfe9 Easy \u2714\ufe0f Loop through all lessons and ensure all skill names are unique.","title":"5. Duplicate Skills Per Lesson"},{"location":"components/universities-curriculum/testing/#6-duplicate-skills-across-university","text":"Goal: Detect redundant skill entries across multiple lessons in the same university. Method(s): Use get_skills_for_lesson() Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Count how often each skill appears and detect unnecessary repetition.","title":"6. Duplicate Skills Across University"},{"location":"components/universities-curriculum/testing/#consistency-testing","text":"","title":"\ud83d\udcd0 Consistency Testing"},{"location":"components/universities-curriculum/testing/#7-cross-course-skill-similarity","text":"Goal: Ensure that related lessons (e.g., under the same department or semester) share logically consistent skill sets. Method(s): Compare lesson skill sets using similarity scoring. Difficulty: \ud83d\udfe5 Hard \u2714\ufe0f Flag anomalies where unrelated skill sets appear within similar contexts.","title":"7. Cross-Course Skill Similarity"},{"location":"components/universities-curriculum/testing/#8-inconsistent-skill-distribution","text":"Goal: Detect universities where similar lesson names have completely disjoint skills. Method(s): Cluster lessons and analyze skill overlap. Difficulty: \ud83d\udfe5 Hard \u2714\ufe0f Suggest possible errors or inconsistencies in extraction.","title":"8. Inconsistent Skill Distribution"},{"location":"components/universities-curriculum/testing/#completeness-testing","text":"","title":"\ud83d\udcca Completeness Testing"},{"location":"components/universities-curriculum/testing/#9-missing-skills-in-lessons","text":"Goal: Flag any lessons with zero skills after processing. Method(s): calculate_skillnames() Difficulty: \ud83d\udfe9 Easy \u2714\ufe0f Useful to catch bad input data or failed OCR.","title":"9. Missing Skills in Lessons"},{"location":"components/universities-curriculum/testing/#10-minimum-skill-threshold","text":"Goal: Validate that each lesson has at least N skills (e.g., 3). Method(s): Compare count from skill output. Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Helps surface under-analyzed lessons.","title":"10. Minimum Skill Threshold"},{"location":"components/universities-curriculum/testing/#accuracy-testing-advanced","text":"","title":"\ud83c\udfaf Accuracy Testing (Advanced)"},{"location":"components/universities-curriculum/testing/#11-skill-mapping-validation-with-esco-labels","text":"Goal: Compare actual ESCO preferred labels with what was extracted. Method(s): Use extract_and_get_title() Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Detect mismatches between ESCO URLs and their resolved names.","title":"11. Skill Mapping Validation with ESCO Labels"},{"location":"components/universities-curriculum/testing/#12-api-json-schema-validation","text":"Goal: Ensure the API responses match expected structure and required fields. Method(s): FastAPI schema + pydantic Difficulty: \ud83d\udfe8 Medium \u2714\ufe0f Useful for future-proofing API integrations.","title":"12. API JSON Schema Validation"},{"location":"components/universities-curriculum/testing/#how-to-run-tests","text":"Install requirements: pip install -r requirements.txt Run all tests: pytest tests/ Run a specific test: pytest tests/test_compare_extracted_skills.py","title":"\u25b6\ufe0f How to Run Tests"},{"location":"components/universities-curriculum/testing/#summary-table","text":"Test Case Type Difficulty Title extraction \ud83c\udfaf Accuracy \ud83d\udfe9 Easy Skill extraction from descriptions \ud83c\udfaf Accuracy \ud83d\udfe8 Medium Top skills per university \ud83c\udfaf Accuracy \ud83d\udfe9 Easy Skill-based search \ud83c\udfaf Accuracy \ud83d\udfe8 Medium Detect duplicate skills \u2705 Data Integrity \ud83d\udfe9 Easy Duplicate across university \u2705 Data Integrity \ud83d\udfe8 Medium Cross-course similarity \ud83d\udcd0 Consistency \ud83d\udfe5 Hard Inconsistent skill distribution \ud83d\udcd0 Consistency \ud83d\udfe5 Hard Missing skills \ud83d\udcca Completeness \ud83d\udfe9 Easy Minimum skills per lesson \ud83d\udcca Completeness \ud83d\udfe8 Medium ESCO label mismatch \ud83c\udfaf Accuracy \ud83d\udfe8 Medium API schema check \ud83c\udfaf Accuracy \ud83d\udfe8 Medium","title":"\ud83e\udde0 Summary Table"}]}